# Tools for PDFs

## Extraction



Automate the boring stuff has a pretty good chapter on using Python to work with PDF documents. Here are some more options. 

If it has tables:
* Tabula
* PDF Plumber: Really easy to use
* CometDocs (need to purchase)

if it has plain text:
* DocumentCloud: Enables you to search text in your documents
* SmallPDF
* Zamzar website 

https://github.com/jsfenfen/parsing-prickly-pdfs

If you have many files and you intend to download all the text afterwards, Overview will let you get a CSV out with one row per document, without writing any code, using the export button.


They say they’re built to get forms-based data just like this. If you end up trying them please report back!
 
http://captricity.com/


I've successfully used Datawatch Monarch for years to handle files like this.. It allows you, via a gui, to design an extraction template based on the relative location of fields, or based on the labels. The documentation is excellent and if you spend a half-day doing the tutorials that come with it, you can learn it pretty quickly.

The problem is the software has gone from being expensive to VERY expensive....


There are programs to extract text and even structure from a PDF...Tabula and ABBYY, for example.


Doesn't Cogniview, which you were the first to recommend, do something similar in that you select what you want to use from a recurring pattern and flattens it into a table? 


The other approach I’m in the process of attempting is converting to HTML so I can use import.io to parse that.

Third option on the approach you’re trying: check http://www.scrapinghub.com. Free and in my experience, far better than import.io in the GUI to control what you get on the scrape.


